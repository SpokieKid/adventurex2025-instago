# Ollama åµŒå…¥æœåŠ¡éƒ¨ç½²æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ Docker Compose éƒ¨ç½² Ollama æœåŠ¡æ¥è§£å†³å‘é‡æ•°æ®åº“çš„åµŒå…¥é—®é¢˜ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ Ollama æœåŠ¡

```bash
docker-compose up -d ollama

services:
  ollama:
    image: ollama/ollama:latest
    container_name: instago-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
   ollama_data:
    driver: local
```

### 2. æ‹‰å–åµŒå…¥æ¨¡å‹

```bash
# æ‹‰å–æ¨èçš„åµŒå…¥æ¨¡å‹
docker exec instago-ollama ollama pull nomic-embed-text
```

### 3. æµ‹è¯•åµŒå…¥æœåŠ¡

```bash
# æµ‹è¯• API æ˜¯å¦æ­£å¸¸å·¥ä½œ
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "æµ‹è¯•æ–‡æœ¬åµŒå…¥"
}'
```

### 4. å¯åŠ¨ Go åº”ç”¨

```bash
cd go-client
go run main.go helpers.go
```

## ğŸ“‹ é…ç½®è¯´æ˜

### Docker Compose é…ç½®

- **ç«¯å£**: Ollama æœåŠ¡è¿è¡Œåœ¨ `11434` ç«¯å£
- **æ•°æ®æŒä¹…åŒ–**: æ¨¡å‹æ•°æ®å­˜å‚¨åœ¨ `ollama_data` å·ä¸­
- **å¥åº·æ£€æŸ¥**: è‡ªåŠ¨æ£€æŸ¥æœåŠ¡çŠ¶æ€
- **GPU æ”¯æŒ**: å¯é€‰çš„ GPU é…ç½®ï¼ˆéœ€è¦ NVIDIA Docker æ”¯æŒï¼‰

### åµŒå…¥æ¨¡å‹é€‰æ‹©

| æ¨¡å‹åç§° | ç»´åº¦ | å¤§å° | ç‰¹ç‚¹ |
|---------|------|------|------|
| `nomic-embed-text` | 768 | ~274MB | é«˜è´¨é‡æ–‡æœ¬åµŒå…¥ï¼Œæ¨èä½¿ç”¨ |
| `all-minilm` | 384 | ~23MB | è½»é‡çº§æ¨¡å‹ï¼Œé€Ÿåº¦å¿« |
| `mxbai-embed-large` | 1024 | ~669MB | å¤§å‹æ¨¡å‹ï¼Œç²¾åº¦æ›´é«˜ |

## ğŸ”§ Go ä»£ç é›†æˆ

ä»£ç å·²è‡ªåŠ¨é…ç½®ä½¿ç”¨ Ollama åµŒå…¥å‡½æ•°ï¼š

```go
// ä½¿ç”¨ Ollama åµŒå…¥å‡½æ•°
embeddingFunc = chromem.NewEmbeddingFuncOllama("nomic-embed-text", "http://localhost:11434/api")
```

### åˆ‡æ¢åµŒå…¥æ¨¡å‹

å¦‚æœéœ€è¦ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼Œä¿®æ”¹ `main.go` ä¸­çš„æ¨¡å‹åç§°ï¼š

```go
// ä½¿ç”¨å¤§å‹æ¨¡å‹
embeddingFunc = chromem.NewEmbeddingFuncOllama("mxbai-embed-large", "http://localhost:11434/api")
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Ollama æœåŠ¡æ— æ³•å¯åŠ¨**
   ```bash
   # æ£€æŸ¥ Docker çŠ¶æ€
   docker-compose logs ollama
   
   # é‡å¯æœåŠ¡
   docker-compose restart ollama
   ```

2. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥ç½‘ç»œè¿æ¥
   docker exec instago-ollama ollama list
   
   # æ‰‹åŠ¨æ‹‰å–æ¨¡å‹
   docker exec -it instago-ollama ollama pull nomic-embed-text
   ```

3. **åµŒå…¥ API è°ƒç”¨å¤±è´¥**
   ```bash
   # æ£€æŸ¥æœåŠ¡çŠ¶æ€
   curl http://localhost:11434/api/tags
   
   # æµ‹è¯•åµŒå…¥ API
   curl http://localhost:11434/api/embeddings -d '{
     "model": "nomic-embed-text",
     "prompt": "test"
   }'
   ```

### å›é€€æ–¹æ¡ˆ

å¦‚æœ Ollama æœåŠ¡ä¸å¯ç”¨ï¼Œå¯ä»¥åœ¨ `main.go` ä¸­å¯ç”¨é»˜è®¤åµŒå…¥å‡½æ•°ï¼š

```go
// å›é€€åˆ°é»˜è®¤åµŒå…¥å‡½æ•°
embeddingFunc = chromem.NewEmbeddingFuncDefault()
```

## ğŸ”„ æœåŠ¡ç®¡ç†

```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d ollama

# åœæ­¢æœåŠ¡
docker-compose down

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f ollama

# é‡å¯æœåŠ¡
docker-compose restart ollama

# æ¸…ç†æ•°æ®ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
docker-compose down -v
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### GPU åŠ é€Ÿ

å¦‚æœæœ‰ NVIDIA GPUï¼Œå¯ä»¥å¯ç”¨ GPU æ”¯æŒï¼š

1. å®‰è£… NVIDIA Docker æ”¯æŒ
2. åœ¨ `docker-compose.yml` ä¸­å–æ¶ˆæ³¨é‡Š GPU é…ç½®
3. ä½¿ç”¨ `ollama-gpu` æœåŠ¡æ›¿ä»£ `ollama`

### å†…å­˜ä¼˜åŒ–

- é€‰æ‹©åˆé€‚å¤§å°çš„åµŒå…¥æ¨¡å‹
- æ ¹æ®æœåŠ¡å™¨é…ç½®è°ƒæ•´ Docker å†…å­˜é™åˆ¶
- è€ƒè™‘ä½¿ç”¨æ¨¡å‹é‡åŒ–ç‰ˆæœ¬

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker stats instago-ollama

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker logs -f instago-ollama

# æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
docker exec instago-ollama ollama list
```

---

**æ³¨æ„**: é¦–æ¬¡å¯åŠ¨æ—¶éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ã€‚è¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®šã€‚